{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90dca834-5049-4724-a467-caaacd1ca11c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\administrateur\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (4.40.1)\n",
      "Collecting datasets\n",
      "  Downloading datasets-2.19.0-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: torch in c:\\users\\administrateur\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (2.2.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\administrateur\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in c:\\users\\administrateur\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from transformers) (0.23.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\administrateur\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from transformers) (1.23.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\administrateur\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\administrateur\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\administrateur\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from transformers) (2023.12.25)\n",
      "Requirement already satisfied: requests in c:\\users\\administrateur\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in c:\\users\\administrateur\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\administrateur\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from transformers) (0.4.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\administrateur\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from transformers) (4.64.0)\n",
      "Collecting pyarrow>=12.0.0 (from datasets)\n",
      "  Downloading pyarrow-16.0.0-cp38-cp38-win_amd64.whl.metadata (3.1 kB)\n",
      "Collecting pyarrow-hotfix (from datasets)\n",
      "  Downloading pyarrow_hotfix-0.6-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
      "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: pandas in c:\\users\\administrateur\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from datasets) (2.0.3)\n",
      "Collecting xxhash (from datasets)\n",
      "  Downloading xxhash-3.4.1-cp38-cp38-win_amd64.whl.metadata (12 kB)\n",
      "Collecting multiprocess (from datasets)\n",
      "  Downloading multiprocess-0.70.16-py38-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: fsspec<=2024.3.1,>=2023.1.0 in c:\\users\\administrateur\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from fsspec[http]<=2024.3.1,>=2023.1.0->datasets) (2023.12.2)\n",
      "Collecting aiohttp (from datasets)\n",
      "  Downloading aiohttp-3.9.5-cp38-cp38-win_amd64.whl.metadata (7.7 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\administrateur\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from torch) (4.9.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\administrateur\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\administrateur\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\administrateur\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from torch) (3.1.3)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp->datasets)\n",
      "  Downloading aiosignal-1.3.1-py3-none-any.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\administrateur\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from aiohttp->datasets) (23.1.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->datasets)\n",
      "  Downloading frozenlist-1.4.1-cp38-cp38-win_amd64.whl.metadata (12 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets)\n",
      "  Downloading multidict-6.0.5-cp38-cp38-win_amd64.whl.metadata (4.3 kB)\n",
      "Collecting yarl<2.0,>=1.0 (from aiohttp->datasets)\n",
      "  Downloading yarl-1.9.4-cp38-cp38-win_amd64.whl.metadata (32 kB)\n",
      "Collecting async-timeout<5.0,>=4.0 (from aiohttp->datasets)\n",
      "  Downloading async_timeout-4.0.3-py3-none-any.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\administrateur\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from requests->transformers) (3.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\administrateur\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\administrateur\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from requests->transformers) (2.0.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\administrateur\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from requests->transformers) (2023.7.22)\n",
      "Requirement already satisfied: colorama in c:\\users\\administrateur\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\administrateur\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\administrateur\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\administrateur\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from pandas->datasets) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\administrateur\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\administrateur\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from sympy->torch) (1.3.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\administrateur\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Downloading datasets-2.19.0-py3-none-any.whl (542 kB)\n",
      "   ---------------------------------------- 0.0/542.0 kB ? eta -:--:--\n",
      "   --------------------------------------  532.5/542.0 kB 16.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 542.0/542.0 kB 8.6 MB/s eta 0:00:00\n",
      "Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "   ---------------------------------------- 0.0/116.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 116.3/116.3 kB 6.6 MB/s eta 0:00:00\n",
      "Downloading aiohttp-3.9.5-cp38-cp38-win_amd64.whl (373 kB)\n",
      "   ---------------------------------------- 0.0/373.1 kB ? eta -:--:--\n",
      "   --------------------------------------- 373.1/373.1 kB 24.2 MB/s eta 0:00:00\n",
      "Downloading pyarrow-16.0.0-cp38-cp38-win_amd64.whl (25.9 MB)\n",
      "   ---------------------------------------- 0.0/25.9 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 1.5/25.9 MB 31.8 MB/s eta 0:00:01\n",
      "   ---- ----------------------------------- 2.9/25.9 MB 31.3 MB/s eta 0:00:01\n",
      "   ------- -------------------------------- 4.6/25.9 MB 32.8 MB/s eta 0:00:01\n",
      "   --------- ------------------------------ 5.9/25.9 MB 31.4 MB/s eta 0:00:01\n",
      "   ----------- ---------------------------- 7.2/25.9 MB 30.6 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 8.6/25.9 MB 30.5 MB/s eta 0:00:01\n",
      "   --------------- ------------------------ 9.9/25.9 MB 30.2 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 11.3/25.9 MB 29.7 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 12.7/25.9 MB 29.7 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 14.0/25.9 MB 28.4 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 15.5/25.9 MB 29.7 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 16.9/25.9 MB 29.7 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 18.3/25.9 MB 29.8 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 19.7/25.9 MB 31.2 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 21.2/25.9 MB 31.1 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 23.1/25.9 MB 31.2 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 24.7/25.9 MB 32.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  25.9/25.9 MB 32.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  25.9/25.9 MB 32.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  25.9/25.9 MB 32.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 25.9/25.9 MB 21.9 MB/s eta 0:00:00\n",
      "Downloading multiprocess-0.70.16-py38-none-any.whl (132 kB)\n",
      "   ---------------------------------------- 0.0/132.6 kB ? eta -:--:--\n",
      "   ---------------------------------------- 132.6/132.6 kB 3.9 MB/s eta 0:00:00\n",
      "Downloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n",
      "Downloading xxhash-3.4.1-cp38-cp38-win_amd64.whl (29 kB)\n",
      "Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Downloading async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
      "Downloading frozenlist-1.4.1-cp38-cp38-win_amd64.whl (50 kB)\n",
      "   ---------------------------------------- 0.0/50.8 kB ? eta -:--:--\n",
      "   ---------------------------------------- 50.8/50.8 kB 2.5 MB/s eta 0:00:00\n",
      "Downloading multidict-6.0.5-cp38-cp38-win_amd64.whl (28 kB)\n",
      "Downloading yarl-1.9.4-cp38-cp38-win_amd64.whl (77 kB)\n",
      "   ---------------------------------------- 0.0/77.1 kB ? eta -:--:--\n",
      "   ---------------------------------------- 77.1/77.1 kB 2.2 MB/s eta 0:00:00\n",
      "Installing collected packages: xxhash, pyarrow-hotfix, pyarrow, multidict, frozenlist, dill, async-timeout, yarl, multiprocess, aiosignal, aiohttp, datasets\n",
      "Successfully installed aiohttp-3.9.5 aiosignal-1.3.1 async-timeout-4.0.3 datasets-2.19.0 dill-0.3.8 frozenlist-1.4.1 multidict-6.0.5 multiprocess-0.70.16 pyarrow-16.0.0 pyarrow-hotfix-0.6 xxhash-3.4.1 yarl-1.9.4\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers datasets torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e336ed72-2a31-4efc-9af5-b0859b071eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, Trainer, TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "54ae96ba-3fef-4999-9555-79ff107712f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset('csv', data_files='dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "63929de1-a4c0-4447-8fba-c4bc13e83f3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': ['ibm', 'tata consultancy services', 'accenture', 'us army', 'ey'],\n",
       " 'keywords': ['ibm,information technology and services,new york, new york, united states,united states',\n",
       "  'tata,consultancy,services,information technology and services,bombay, maharashtra, india,india',\n",
       "  'accenture,information technology and services,dublin, dublin, ireland,ireland',\n",
       "  'us,army,military,alexandria, virginia, united states,united states',\n",
       "  'ey,accounting,london, greater london, united kingdom,united kingdom']}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f020b52b-08b9-4b5a-a896-41fa843acb62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_function(example):\n",
    "    # Check if either 'name' or 'keywords' are None or empty strings\n",
    "    return example['name'] not in (None, '') and example['keywords'] not in (None, '')\n",
    "\n",
    "dataset = dataset.filter(filter_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "12b7cb22-4d68-4df3-b0a0-64a9aca17644",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.0043468475341796875,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 29,
       "postfix": null,
       "prefix": "Map",
       "rate": null,
       "total": 7173423,
       "unit": " examples",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1cc9e0230d1e48e997db827278a07f27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7173423 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def preprocess_function(examples):\n",
    "    # Concatenate 'keywords' and 'name' with a specific format for model training\n",
    "    examples['text'] = examples['keywords'] + \" -> \" + examples['name']\n",
    "    return examples\n",
    "\n",
    "# Apply the preprocessing function to the dataset\n",
    "dataset = dataset.map(preprocess_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2dfa4865-7c2f-46a3-bc94-fcc72f2bf9b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['name', 'keywords', 'text'],\n",
       "        num_rows: 900\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['name', 'keywords', 'text'],\n",
       "        num_rows: 100\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_dataset = dataset['train'].shuffle(seed=42).select(range(1_000))\n",
    "train_test_split = sample_dataset.train_test_split(test_size=0.1)\n",
    "train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a6c2f988-c846-4e9a-91b1-6fbce1cf6e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_test_split['train']\n",
    "test_dataset = train_test_split['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a657ba32-a686-4309-a86b-efc34c78c9ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrateur\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.0019071102142333984,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 29,
       "postfix": null,
       "prefix": "Map",
       "rate": null,
       "total": 900,
       "unit": " examples",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62da3dee299544c1b99ab6fdf9343153",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/900 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.01645040512084961,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 29,
       "postfix": null,
       "prefix": "Map",
       "rate": null,
       "total": 100,
       "unit": " examples",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03ba01abbeed4f2990256d09d2270b22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    # Tokenizing the text data for GPT-2 model input\n",
    "    model_inputs = tokenizer(examples['text'], padding=\"max_length\", truncation=True, max_length=128)\n",
    "    \n",
    "    # GPT-2 expects labels for calculating the loss; we use input_ids as labels for language modeling.\n",
    "    model_inputs[\"labels\"] = model_inputs[\"input_ids\"].copy()\n",
    "    return model_inputs\n",
    "\n",
    "# Apply tokenization and prepare the dataset\n",
    "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "test_dataset = test_dataset.map(tokenize_function, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "932c20ac-f50b-45a4-a6a0-bd1a9ae85087",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'zdravotnický holding královéhradeckého kraje a.s.',\n",
       " 'keywords': 'zdravotnický,holding,královéhradeckého,kraje,a.s.,hospital & health care,hradec kralove, kralovehradecky kraj, czechia,czechia',\n",
       " 'text': 'zdravotnický,holding,královéhradeckého,kraje,a.s.,hospital & health care,hradec kralove, kralovehradecky kraj, czechia,czechia -> zdravotnický holding královéhradeckého kraje a.s.',\n",
       " 'input_ids': [89,\n",
       "  67,\n",
       "  4108,\n",
       "  313,\n",
       "  17172,\n",
       "  127,\n",
       "  121,\n",
       "  11,\n",
       "  19216,\n",
       "  11,\n",
       "  38584,\n",
       "  6557,\n",
       "  27086,\n",
       "  2634,\n",
       "  11840,\n",
       "  671,\n",
       "  694,\n",
       "  2634,\n",
       "  8873,\n",
       "  11,\n",
       "  74,\n",
       "  430,\n",
       "  18015,\n",
       "  11,\n",
       "  64,\n",
       "  13,\n",
       "  82,\n",
       "  1539,\n",
       "  49257,\n",
       "  1222,\n",
       "  1535,\n",
       "  1337,\n",
       "  11,\n",
       "  11840,\n",
       "  671,\n",
       "  66,\n",
       "  479,\n",
       "  1373,\n",
       "  659,\n",
       "  11,\n",
       "  479,\n",
       "  1373,\n",
       "  659,\n",
       "  11840,\n",
       "  671,\n",
       "  694,\n",
       "  88,\n",
       "  479,\n",
       "  430,\n",
       "  73,\n",
       "  11,\n",
       "  269,\n",
       "  15356,\n",
       "  544,\n",
       "  11,\n",
       "  66,\n",
       "  15356,\n",
       "  544,\n",
       "  4613,\n",
       "  1976,\n",
       "  67,\n",
       "  4108,\n",
       "  313,\n",
       "  17172,\n",
       "  127,\n",
       "  121,\n",
       "  4769,\n",
       "  479,\n",
       "  81,\n",
       "  6557,\n",
       "  27086,\n",
       "  2634,\n",
       "  11840,\n",
       "  671,\n",
       "  694,\n",
       "  2634,\n",
       "  8873,\n",
       "  479,\n",
       "  430,\n",
       "  18015,\n",
       "  257,\n",
       "  13,\n",
       "  82,\n",
       "  13,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256],\n",
       " 'attention_mask': [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " 'labels': [89,\n",
       "  67,\n",
       "  4108,\n",
       "  313,\n",
       "  17172,\n",
       "  127,\n",
       "  121,\n",
       "  11,\n",
       "  19216,\n",
       "  11,\n",
       "  38584,\n",
       "  6557,\n",
       "  27086,\n",
       "  2634,\n",
       "  11840,\n",
       "  671,\n",
       "  694,\n",
       "  2634,\n",
       "  8873,\n",
       "  11,\n",
       "  74,\n",
       "  430,\n",
       "  18015,\n",
       "  11,\n",
       "  64,\n",
       "  13,\n",
       "  82,\n",
       "  1539,\n",
       "  49257,\n",
       "  1222,\n",
       "  1535,\n",
       "  1337,\n",
       "  11,\n",
       "  11840,\n",
       "  671,\n",
       "  66,\n",
       "  479,\n",
       "  1373,\n",
       "  659,\n",
       "  11,\n",
       "  479,\n",
       "  1373,\n",
       "  659,\n",
       "  11840,\n",
       "  671,\n",
       "  694,\n",
       "  88,\n",
       "  479,\n",
       "  430,\n",
       "  73,\n",
       "  11,\n",
       "  269,\n",
       "  15356,\n",
       "  544,\n",
       "  11,\n",
       "  66,\n",
       "  15356,\n",
       "  544,\n",
       "  4613,\n",
       "  1976,\n",
       "  67,\n",
       "  4108,\n",
       "  313,\n",
       "  17172,\n",
       "  127,\n",
       "  121,\n",
       "  4769,\n",
       "  479,\n",
       "  81,\n",
       "  6557,\n",
       "  27086,\n",
       "  2634,\n",
       "  11840,\n",
       "  671,\n",
       "  694,\n",
       "  2634,\n",
       "  8873,\n",
       "  479,\n",
       "  430,\n",
       "  18015,\n",
       "  257,\n",
       "  13,\n",
       "  82,\n",
       "  13,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256]}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "77203020-f7be-4532-be74-46ff86fffd4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='675' max='675' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [675/675 57:32, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.532500</td>\n",
       "      <td>0.472040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.406200</td>\n",
       "      <td>0.441925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.399200</td>\n",
       "      <td>0.434958</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['lm_head.weight'].\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=675, training_loss=0.5309929900699192, metrics={'train_runtime': 3457.0803, 'train_samples_per_second': 0.781, 'train_steps_per_second': 0.195, 'total_flos': 176372121600000.0, 'train_loss': 0.5309929900699192, 'epoch': 3.0})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    evaluation_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d39d65e2-68c3-4239-a94c-718806442768",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13/13 00:34]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.43495845794677734, 'eval_runtime': 36.13, 'eval_samples_per_second': 2.768, 'eval_steps_per_second': 0.36, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "# Save the model and tokenizer\n",
    "model.save_pretrained('./trained_model')\n",
    "tokenizer.save_pretrained('./trained_model')\n",
    "\n",
    "# Evaluate the model on the validation set\n",
    "results = trainer.evaluate()\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "090303bc-6212-49d4-9e27-b5e3df105dc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text: Innovation, Tech, Advancement, AI, DeepLearning, Machine Learning -> ininnuine\n"
     ]
    }
   ],
   "source": [
    "def generate_text(prompt_text, max_length=50):\n",
    "    # Encode the prompts using the tokenizer\n",
    "    encoded_prompt = tokenizer.encode(prompt_text, add_special_tokens=False, return_tensors=\"pt\")\n",
    "    \n",
    "    # Generate a sequence of tokens following the prompt\n",
    "    output_sequences = model.generate(\n",
    "        input_ids=encoded_prompt,\n",
    "        max_length=max_length + len(encoded_prompt[0]),\n",
    "        temperature=1.0,\n",
    "        top_k=40,\n",
    "        top_p=0.95,\n",
    "        repetition_penalty=1.2,\n",
    "        do_sample=True,\n",
    "        num_return_sequences=1\n",
    "    )\n",
    "    \n",
    "    # Decode the output sequences to strings\n",
    "    generated_sequences = []\n",
    "    for generated_sequence_idx, generated_sequence in enumerate(output_sequences):\n",
    "        generated_sequence = generated_sequence.tolist()\n",
    "        \n",
    "        # Decode text\n",
    "        text = tokenizer.decode(generated_sequence, clean_up_tokenization_spaces=True)\n",
    "        \n",
    "        # Remove all text after the stop token\n",
    "        text = text[: text.find(tokenizer.eos_token)]\n",
    "        \n",
    "        generated_sequences.append(text)\n",
    "    \n",
    "    return generated_sequences[0]\n",
    "\n",
    "# Example usage\n",
    "prompt_text = \"Innovation, Tech, Advancement, AI, DeepLearning, Machine Learning\"\n",
    "generated_text = generate_text(prompt_text + ' ->')\n",
    "print(\"Generated Text:\", generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d7fd951-cd31-4228-ad78-4dce96696bc0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
